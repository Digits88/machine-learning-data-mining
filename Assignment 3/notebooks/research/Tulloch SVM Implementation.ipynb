{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import cvxopt.solvers\n",
    "import logging\n",
    "\n",
    "\n",
    "MIN_SUPPORT_VECTOR_MULTIPLIER = 1e-5\n",
    "\n",
    "\n",
    "class SVMTrainer(object):\n",
    "    def __init__(self, kernel, c):\n",
    "        self._kernel = kernel\n",
    "        self._c = c\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"Given the training features X with labels y, returns a SVM\n",
    "        predictor representing the trained SVM.\n",
    "        \"\"\"\n",
    "        lagrange_multipliers = self._compute_multipliers(X, y)\n",
    "        return self._construct_predictor(X, y, lagrange_multipliers)\n",
    "\n",
    "    def _gram_matrix(self, X):\n",
    "        n_samples, n_features = X.shape\n",
    "        K = np.zeros((n_samples, n_samples))\n",
    "        # TODO(tulloch) - vectorize\n",
    "        for i, x_i in enumerate(X):\n",
    "            for j, x_j in enumerate(X):\n",
    "                K[i, j] = self._kernel(x_i, x_j)\n",
    "        return K\n",
    "\n",
    "    def _construct_predictor(self, X, y, lagrange_multipliers):\n",
    "        support_vector_indices = \\\n",
    "            lagrange_multipliers > MIN_SUPPORT_VECTOR_MULTIPLIER\n",
    "\n",
    "        support_multipliers = lagrange_multipliers[support_vector_indices]\n",
    "        support_vectors = X[support_vector_indices]\n",
    "        support_vector_labels = y[support_vector_indices]\n",
    "\n",
    "        # http://www.cs.cmu.edu/~guestrin/Class/10701-S07/Slides/kernels.pdf\n",
    "        # bias = y_k - \\sum z_i y_i  K(x_k, x_i)\n",
    "        # Thus we can just predict an example with bias of zero, and\n",
    "        # compute error.\n",
    "        bias = np.mean(\n",
    "            [y_k - SVMPredictor(\n",
    "                kernel=self._kernel,\n",
    "                bias=0.0,\n",
    "                weights=support_multipliers,\n",
    "                support_vectors=support_vectors,\n",
    "                support_vector_labels=support_vector_labels).predict(x_k)\n",
    "             for (y_k, x_k) in zip(support_vector_labels, support_vectors)])\n",
    "\n",
    "        return SVMPredictor(\n",
    "            kernel=self._kernel,\n",
    "            bias=bias,\n",
    "            weights=support_multipliers,\n",
    "            support_vectors=support_vectors,\n",
    "            support_vector_labels=support_vector_labels)\n",
    "\n",
    "    def _compute_multipliers(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        K = self._gram_matrix(X)\n",
    "        # Solves\n",
    "        # min 1/2 x^T P x + q^T x\n",
    "        # s.t.\n",
    "        #  Gx \\coneleq h\n",
    "        #  Ax = b\n",
    "\n",
    "        P = cvxopt.matrix(np.outer(y, y) * K)\n",
    "        q = cvxopt.matrix(-1 * np.ones(n_samples))\n",
    "\n",
    "        # -a_i \\leq 0\n",
    "        # TODO(tulloch) - modify G, h so that we have a soft-margin classifier\n",
    "        G_std = cvxopt.matrix(np.diag(np.ones(n_samples) * -1))\n",
    "        h_std = cvxopt.matrix(np.zeros(n_samples))\n",
    "\n",
    "        # a_i \\leq c\n",
    "        G_slack = cvxopt.matrix(np.diag(np.ones(n_samples)))\n",
    "        h_slack = cvxopt.matrix(np.ones(n_samples) * self._c)\n",
    "\n",
    "        G = cvxopt.matrix(np.vstack((G_std, G_slack)))\n",
    "        h = cvxopt.matrix(np.vstack((h_std, h_slack)))\n",
    "\n",
    "        A = cvxopt.matrix(y, (1, n_samples))\n",
    "        b = cvxopt.matrix(0.0)\n",
    "\n",
    "        solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "\n",
    "        # Lagrange multipliers\n",
    "        r = np.ravel(solution['x'])\n",
    "        \n",
    "        print r\n",
    "        \n",
    "        return r\n",
    "\n",
    "\n",
    "class SVMPredictor(object):\n",
    "    def __init__(self,\n",
    "                 kernel,\n",
    "                 bias,\n",
    "                 weights,\n",
    "                 support_vectors,\n",
    "                 support_vector_labels):\n",
    "        self._kernel = kernel\n",
    "        self._bias = bias\n",
    "        self._weights = weights\n",
    "        self._support_vectors = support_vectors\n",
    "        self._support_vector_labels = support_vector_labels\n",
    "        assert len(support_vectors) == len(support_vector_labels)\n",
    "        assert len(weights) == len(support_vector_labels)\n",
    "        logging.info(\"Bias: %s\", self._bias)\n",
    "        logging.info(\"Weights: %s\", self._weights)\n",
    "        logging.info(\"Support vectors: %s\", self._support_vectors)\n",
    "        logging.info(\"Support vector labels: %s\", self._support_vector_labels)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Computes the SVM prediction on the given features x.\n",
    "        \"\"\"\n",
    "        result = self._bias\n",
    "        for z_i, x_i, y_i in zip(self._weights,\n",
    "                                 self._support_vectors,\n",
    "                                 self._support_vector_labels):\n",
    "            result += z_i * y_i * self._kernel(x_i, x)\n",
    "        return np.sign(result).item()\n",
    "\n",
    "class Kernel(object):\n",
    "    \"\"\"Implements list of kernels from\n",
    "    http://en.wikipedia.org/wiki/Support_vector_machine\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def linear():\n",
    "        return lambda x, y: np.inner(x, y)\n",
    "\n",
    "    @staticmethod\n",
    "    def gaussian(sigma):\n",
    "        return lambda x, y: \\\n",
    "            np.exp(-np.sqrt(la.norm(x-y) ** 2 / (2 * sigma ** 2)))\n",
    "\n",
    "    @staticmethod\n",
    "    def _polykernel(dimension, offset):\n",
    "        return lambda x, y: (offset + np.inner(x, y)) ** dimension\n",
    "\n",
    "    @classmethod\n",
    "    def inhomogenous_polynomial(cls, dimension):\n",
    "        return cls._polykernel(dimension=dimension, offset=1.0)\n",
    "\n",
    "    @classmethod\n",
    "    def homogenous_polynomial(cls, dimension):\n",
    "        return cls._polykernel(dimension=dimension, offset=0.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def hyperbolic_tangent(kappa, c):\n",
    "        return lambda x, y: np.tanh(kappa * np.dot(x, y) + c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.6281e+00 -1.8386e+00  4e+01  6e+00  2e-16\n",
      " 1: -5.8755e-01 -1.6773e+00  3e+00  3e-01  5e-16\n",
      " 2: -4.7879e-01 -8.1803e-01  3e-01  6e-17  4e-16\n",
      " 3: -5.1624e-01 -5.6147e-01  5e-02  7e-17  3e-16\n",
      " 4: -5.3324e-01 -5.4005e-01  7e-03  5e-17  2e-16\n",
      " 5: -5.3662e-01 -5.3705e-01  4e-04  6e-17  2e-16\n",
      " 6: -5.3679e-01 -5.3680e-01  1e-05  3e-17  2e-16\n",
      " 7: -5.3679e-01 -5.3679e-01  1e-07  3e-17  3e-16\n",
      "Optimal solution found.\n",
      "[  9.99999992e-02   1.17668920e-08   8.17242078e-09   8.31253918e-02\n",
      "   9.99999990e-02   9.99999813e-02   9.99999991e-02   1.68746263e-02\n",
      "   1.26317323e-08   9.99999653e-02]\n",
      "[[-1.06083048  0.39802356]\n",
      " [ 0.50550056  0.93733807]\n",
      " [ 0.27138463 -1.17178827]\n",
      " [ 0.12903297  0.35697593]\n",
      " [-0.07114667 -0.50193906]\n",
      " [-0.4351612   1.57280337]\n",
      " [ 0.65395017  0.26906029]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAAD7CAYAAAChScXIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2gAQEEIbAAJCaANAQAhtAAgIoQ0AAfl/djTv6nB5YNEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109f32510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import itertools\n",
    "\n",
    "def plot(predictor, X, y, grid_size):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, grid_size),\n",
    "                         np.linspace(y_min, y_max, grid_size),\n",
    "                         indexing='ij')\n",
    "    flatten = lambda m: np.array(m).reshape(-1,)\n",
    "\n",
    "    result = []\n",
    "    for (i, j) in itertools.product(range(grid_size), range(grid_size)):\n",
    "        point = np.array([xx[i, j], yy[i, j]]).reshape(1, 2)\n",
    "        result.append(predictor.predict(point))\n",
    "\n",
    "    Z = np.array(result).reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(xx, yy, Z,\n",
    "                 cmap=cm.Paired,\n",
    "                 levels=[-0.001, 0.001],\n",
    "                 extend='both',\n",
    "                 alpha=0.8)\n",
    "    plt.scatter(flatten(X[:, 0]), flatten(X[:, 1]),\n",
    "                c=flatten(y), cmap=cm.Paired)\n",
    "    \n",
    "    print predictor._support_vectors\n",
    "    \n",
    "    svx, svy = [], []\n",
    "    \n",
    "    for s in predictor._support_vectors:\n",
    "        svx.append(s[0, 0])\n",
    "        svy.append(s[0, 1])\n",
    "    \n",
    "    plt.scatter(svx, svy, s=30)\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    \n",
    "num_samples=10\n",
    "num_features=2\n",
    "grid_size=20\n",
    "\n",
    "samples = np.matrix(np.random.normal(size=num_samples * num_features)\n",
    "                    .reshape(num_samples, num_features))\n",
    "labels = 2 * (samples.sum(axis=1) > 0) - 1.0\n",
    "trainer = SVMTrainer(Kernel.linear(), 0.1)\n",
    "predictor = trainer.train(samples, labels)\n",
    "\n",
    "plot(predictor, samples, labels, grid_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
